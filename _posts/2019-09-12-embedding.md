---
title: 한국어 임베딩
category: Natural Language Processing
tag: embedding
---

**임베딩(embedding)**은 자연어를 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 가리키는 용어입니다. 단어나 문장 각각을 벡터로 변환해 벡터 공간에 '끼워 넣는다(embed)'는 취지에서 임베딩이라는 이름이 붙었습니다. 컴퓨터가 자연어를 처리할 수 있게 하려면 자연어를 계산 가능한 형식인 임베딩으로 바꿔줘야 합니다. 

임베딩은 컴퓨터가 자연어를 이해하도록 하는 첫 관문으로 매우 중요한 기능을 합니다. 자연어 처리 모델의 성능은 임베딩이 좌우한다고 해도 과언이 아닌데요. 제가 이번에 많은 시간과 노력을 들여서 [한국어 임베딩](http://www.yes24.com/Product/Goods/78569687)이라는 책을 펴냈습니다. 이 책에서는 다양한 임베딩 기법을 일별하고 한국어 데이터 전처리, 임베딩 구축에 이르는 전 과정을 튜토리얼 방식으로 소개합니다. 

다음 그림을 클릭하면 도서 안내 페이지로 이동합니다.

<a href="http://www.yes24.com/Product/Goods/78569687"><img src="https://i.imgur.com/j03ENCc.jpg" width="500px" title="embeddings" /></a>





## 임베딩이 중요한 이유

임베딩에는 말뭉치(corpus)의 의미, 문법 정보가 응축돼 있습니다. 임베딩은 벡터이기 때문에 사칙연산이 가능하며, 단어/문서 관련도(relevance) 역시 계산할 수 있습니다. 

최근 들어 임베딩이 중요해진 이유는 따로 있습니다. 바로 전이 학습(transfer learning) 때문입니다. 전이 학습이란 특정 문제를 풀기 위해 학습한 모델을 다른 문제를 푸는 데 재사용하는 기법을 의미합니다. 예컨대 대규모 말뭉치를 미리 학습(pretrain)한 임베딩을 문서 분류 모델의 입력값으로 쓰고, 해당 임베딩을 포함한 모델 전체를 문서 분류 과제를 잘할 수 있도록 업데이트(fine-tuning)하는 방식이 바로 그것입니다. 물론 전이 학습은 문서 분류 이외의 다양한 다른 과제에도 적용할 수 있습니다. 

전이 학습 혹은 프리트레인-파인 튜닝 메커니즘은 사람의 학습과 비슷한 점이 있습니다. 사람은 무언가를 배울 때 제로 베이스에서 시작하지 않습니다. 사람이 새로운 사실을 빠르게 이해할 수 있는 이유는 그가 이해를 하는 데에 평생 쌓아 온 지식을 동원하기 때문입니다. 자연어 처리 모델 역시 제로에서 시작하지 않습니다. 우선 대규모 말뭉치를 학습시켜 임베딩을 미리 만들어 놓습니다**(프리트레인)**. 이 임베딩에는 의미, 문법 정보가 녹아 있습니다. 이후 임베딩을 포함한 모델 전체를 문서 분류 과제에 맞게 업데이트합니다**(파인 튜닝)**. 이로써 전이 학습 모델은 제로부터 학습한 모델보다 문서 분류 과제를 빠르게 잘 수행할 수 있습니다. 

다음은 [네이버 영화 리뷰 말뭉치(NSMC)](https://github.com/e9t/nsmc)를 가지고 영화 리뷰 문서의 극성(polarity)을 예측하는 모델의 정확도(accuracy)와 학습 손실(training loss)를 그래프로 나타낸 것입니다. `FastText`는 이 모델의 입력값을 단어 임베딩 기법의 일종인 FastText를 사용한 것이고, `Random`은 랜덤 임베딩을 썼다는 뜻입니다. 후자는 다시 말해 학습을 제로에서부터 시작했다는 뜻입니다. 그래프를 보면 아시겠지만 임베딩 품질이 좋으면 수행하려는 태스크(극성 분류)의 성능이 올라갑니다. 아울러 모델의 수렴(converge) 역시 빨라집니다.



<a href="https://imgur.com/u8yHOqM"><img src="https://i.imgur.com/u8yHOqM.png" title="source: imgur.com" width="500px" /></a>

<a href="https://imgur.com/5R661Va"><img src="https://i.imgur.com/5R661Va.png" width="500px" title="source: imgur.com" /></a>



품질 좋은 임베딩은 잘 담근 김치와 같습니다. 김치 맛이 좋으면 물만 부어 끓인 김치찌개 맛도 좋습니다. 임베딩 품질이 좋으면 단순한 모델로도 원하는 성능을 낼 수 있습니다. 모델 구조가 동일하다면 그 성능은 높고 수렴(converge)은 빠릅니다. 자연어 처리 모델을 만들고 서비스할 때 중요한 구성 요소 하나만 꼽으라고 한다면, 저는 주저하지 않고 ‘임베딩’을 꼽을 것입니다. ELMo(Embeddings from Language Models), BERT(Bidirectional Encoder Representations from Transformer), GPT(Generative Pre-Training) 등 자연어 처리 분야에서 당대 최고 성능을 내는 기법들이 모두 전이 학습 혹은 프리트레인-파인 튜닝 메커니즘을 사용하는 것은 우연의 일치가 아닙니다.





## 이 책이 다루는 범위

[한국어 임베딩](http://www.yes24.com/Product/Goods/78569687)에서는 NPLM(Neural Probabilistic Language Model), Word2Vec, FastText, 잠재 의미 분석(LSA), GloVe, Swivel 등 6가지 단어 수준 임베딩 기법, LSA, Doc2Vec, 잠재 디리클레 할당(LDA), ELMo, BERT 등 5가지 문장 수준 임베딩 기법을 소개합니다. 이외에도 다양한 임베딩 기법이 있지만 두 가지 원칙에 입각해 일부만 골랐습니다. 우선 성능이 안정적이고 뛰어나 현업에 바로 적용해봄직한 기법을 선택했습니다. 또 임베딩 기법의 발전 양상을 이해하는 데 중요한 역할을 하는 모델을 포함했습니다. ‘정보의 홍수’ 속에서 살아가는 독자들에게 핵심에 해당하는 지식만을 전해주고 싶었기 때문입니다. 기타 임베딩 기법들은 대부분, 이 책에서 소개하는 11개 모델의 변형에 해당하기 때문에 독자 여러분이 추가로 공부하고 싶은 최신 기법이 있다면 이 책에서 가지를 쳐 나가는 식으로 학습하면 수월할 거라 생각합니다. 

이와 관련해 XLNet이라는 기법을 짚고 넘어가야겠습니다. XLNet은 구글 연구팀이 2019년 상반기 발표한 모델로, 공개 당시 20개 자연어 처리 데이터셋에서 최고 성능을 기록해 주목받았습니다. 출간을 한 달 정도 늦춰 가며 목차와 내용을 전반적으로 손질할 수밖에 없었습니다. 그러나 직접 실험한 결과 XLNet의 파인 튜닝 성능(분류)이 BERT보다 뒤지는 것은 물론, 동일한 하이퍼파라미터(hyperparameter)로도 점수가 들쭉날쭉한 양상을 보였습니다. XLNet 저자 공식 리포지터리(https://github.com/zihangdai/xlnet)에도 비슷한 사례가 꾸준히 보고되고 있으나 출간 직전인 2019년 9월 현재까지 납득할 만한 해결책이 제시되지 않고 있습니다. 이에 아쉽기는 하지만 XLNet 관련 장을 1판에서는 제외하고 2판 이후를 기약하기로 했습니다. 그럼에도 XLNet을 공부하고 싶은 독자가 있다면 다음 링크를 확인하시면 됩니다.



- [XLNet](https://ratsgo.github.io/natural language processing/2019/09/11/xlnet/)





## 튜토리얼

[한국어 임베딩](http://www.yes24.com/Product/Goods/78569687)은 각 임베딩 기법의 이론적 배경을 설명함과 동시에 공개돼 있는 한국어 말뭉치로 실습하는 것을 목표로 합니다. 이 책의 모든 실습 코드는 다음 깃허브 리포지터리에 공개돼 있습니다. 코드는 예고 없이 수정될 수 있으니 다음 리포지터리의 최신 코드를 받아 실행하기를 권합니다.



● https://github.com/ratsgo/embedding



책에 소개된 코드(특히 bash 스크립트)를 그대로 실행하고 싶은데 일일이 타이핑하기엔 너무 길어 불편할 수 있습니다. 복사해서 붙여 넣기 쉽도록 다음 페이지에 기법별로 스크립트를 정리해 놓았습니다. 



● https://ratsgo.github.io/embedding





## 도서 구매 안내

전국 주요 서점과 온라인으로 구매할 수 있습니다. 온라인 서점 링크는 다음과 같습니다.



- [YES24](http://www.yes24.com/Product/Goods/78569687)
- [교보문고](http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9791161753508)
- [알라딘](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=206404643)